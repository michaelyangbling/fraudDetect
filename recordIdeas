PCA to reduce dimension, since feature is sparse due to too many categories in a categorical feature, when using dummy encoding
Auc cost function or weighted training to deal with imbalanced classes
time can be split into time windows to be categorical, or just use continuous time
wide and deep NN may need detailed tuning

hash categories may help:
As with many counterintuitive phenomena in machine learning, 
it turns out that hashing often works well in practice. 
That's because hash categories provide the model with some separation.


...


Neural Network
When using sample training to do CV (100000 records)
CV result:
128 batch size( about 0.994 auc ) performs better than 1000( about 0.991 auc) batch size.
when using 128 batch size, best CV score until auc decline in validation set ( manual early stopping )
embed ip:
embedding_best: 0.994696502302   no_embedding_best:0.9932


Full data set:
CV result:
embed ip:
每次mini batch 取128个，几分钟随便跑了跑三万轮gradient descent，6万个test data CV出来0.996的auc。。。不知道为啥做CV准确率这么高，


train data from 11-06:
skip=range(5*(10**7),18*(10**7))
train=pd.read_csv( data_path+'train.csv.zip', dtype={'ip':'int32','app':'int16','device':'int16',
                  'os':'int16','channel':'int16','is_attributed':'bool_'}
                    ,parse_dates=['click_time'], usecols=[0,1,2,3,4,5,7],skiprows=skip)

trn=train.head(80000)
tst=train.tail(20000000).head(20000) test on11-07data:auc 0.876842606516

but tst on 11-06 tst=train.head(100000).tail(20000) auc=0.923375897179

This implies time series pattern influences a lot on prediction!
xx
when only using future to test and use past data to train
without embedding:0.954779799124

with embedding:0.94612782134(embedding only ip  speed up 10 times)
result drops!  underlying time series pattern exists in data...

after adding time series features like diffDays and diffSeconds..

sample data~~after adding time series feature: best round at [120]	valid_0's auc: 0.956876
ft
      feature       gain  split
6   time_diff  41.310665  30618
1         app  33.590455    281
4     channel  11.374312    313
5  scaledHour  10.325051   5133
3          os   3.145820    322
0          ip   0.170510    282
2      device   0.080677     46
7     difDays   0.002509      5

sample data~~no time series feature: best round at [90]	valid_0's auc: 0.943544
      feature       gain  split
1         app  41.964600    303
5  scaledHour  36.043675  12939
4     channel  15.405695    317
3          os   5.922034    338
0          ip   0.436242     79
2      device   0.227753     57



embedding: 1500 rounds for shuffle=false
after adding time series features like diffDays and diffSeconds..
NNresult drops from about 0.939875163028 to 0.935816659872...
this is because of feature relevance...
By looking at correlation matrix, it seems that time_diff and difDays are highly relevant

only adding sDiff 0.9323  only adding dayDiff 0.9354

grid search result:
activation: <function elu at 0x1177e06a8>  units:  [70, 50, 30] dropout: 0.1  rounds 1200auc:   0.942316839005
activation: <function relu at 0x117805488>  units:  [50, 25, 10, 5] dropout: 0.5  rounds 1200auc:   0.921379268962
activation: <function relu at 0x117805488>  units:  [40, 30, 20, 10, 5] dropout: 0.5  rounds 1600auc:   0.951233544939
activation: <function relu at 0x117805488>  units:  [30, 20, 10] dropout: 0.1  rounds 1200auc:   0.951631298986
activation: <function elu at 0x1177e06a8>  units:  [100] dropout: 0.5  rounds 1200auc:   0.95684600234
activation: <function elu at 0x1177e06a8>  units:  [10, 10] dropout: 0.1  rounds 1600auc:   0.943892412715
activation: <function relu at 0x117805488>  units:  [35] dropout: 0.5  rounds 1600auc:   0.9289189473
activation: <function elu at 0x1177e06a8>  units:  [50, 25, 10, 5] dropout: 0.2  rounds 1200auc:   0.941277781013
activation: <function elu at 0x1177e06a8>  units:  [80, 40] dropout: 0.5  rounds 2000auc:   0.931897580208
activation: <function relu at 0x117805488>  units:  [35] dropout: 0.1  rounds 1600auc:   0.948554275448
activation: <function relu at 0x117805488>  units:  [160, 60] dropout: 0.2  rounds 1800auc:   0.940716998548
activation: <function relu at 0x117805488>  units:  [70, 50, 30] dropout: 0.1  rounds 1400auc:   0.940472125008
activation: <function relu at 0x117805488>  units:  [80, 40] dropout: 0.1  rounds 1400auc:   0.941952837798
activation: <function elu at 0x1177e06a8>  units:  [40, 30, 20, 10, 5] dropout: 0.1  rounds 1600auc:   0.947459183331
activation: <function relu at 0x117805488>  units:  [10, 10] dropout: 0.5  rounds 1200auc:   0.941064233638
activation: <function elu at 0x1177e06a8>  units:  [80, 40] dropout: 0.2  rounds 1200auc:   0.953078038329
activation: <function elu at 0x1177e06a8>  units:  [200] dropout: 0.5  rounds 2400auc:   0.944448783045
activation: <function elu at 0x1177e06a8>  units:  [50, 25, 10, 5] dropout: 0.5  rounds 3200auc:   0.938038390876
activation: <function relu at 0x117805488>  units:  [200] dropout: 0.5  rounds 1400auc:   0.950736076623
activation: <function elu at 0x1177e06a8>  units:  [65, 50, 35, 20, 10] dropout: 0.2  rounds 1200auc:   0.939215107506
activation: <function elu at 0x1177e06a8>  units:  [70, 50, 30] dropout: 0.2  rounds 1400auc:   0.944881613571
activation: <function relu at 0x117805488>  units:  [70, 50, 30] dropout: 0.5  rounds 1600auc:   0.944070442396
activation: <function elu at 0x1177e06a8>  units:  [65, 50, 35, 20, 10] dropout: 0.5  rounds 1600auc:   0.953790377661
activation: <function relu at 0x117805488>  units:  [65, 50, 35, 20, 10] dropout: 0.1  rounds 1200auc:   0.921037549041
activation: <function relu at 0x117805488>  units:  [10, 10] dropout: 0.2  rounds 1600auc:   0.952122149098
activation: <function elu at 0x1177e06a8>  units:  [35] dropout: 0.1  rounds 2600auc:   0.947418591682
activation: <function elu at 0x1177e06a8>  units:  [40, 30, 20, 10, 5] dropout: 0.5  rounds 1200auc:   0.940619049132
activation: <function relu at 0x117805488>  units:  [65, 50, 35, 20, 10] dropout: 0.2  rounds 1200auc:   0.9445350403
activation: <function elu at 0x1177e06a8>  units:  [65, 50, 35, 20, 10] dropout: 0.1  rounds 1800auc:   0.940093563753
activation: <function relu at 0x117805488>  units:  [200] dropout: 0.2  rounds 1200auc:   0.943321482336
activation: <function relu at 0x117805488>  units:  [50, 25, 10, 5] dropout: 0.2  rounds 1400auc:   0.91698831137
activation: <function elu at 0x1177e06a8>  units:  [160, 60] dropout: 0.5  rounds 1200auc:   0.94078626908
activation: <function relu at 0x117805488>  units:  [100] dropout: 0.1  rounds 1200auc:   0.917147368867
activation: <function elu at 0x1177e06a8>  units:  [30, 20, 10] dropout: 0.5  rounds 1800auc:   0.948323520743
activation: <function elu at 0x1177e06a8>  units:  [30, 20, 10] dropout: 0.1  rounds 1200auc:   0.92107681705
activation: <function elu at 0x1177e06a8>  units:  [35] dropout: 0.5  rounds 1400auc:   0.949677605234
activation: <function elu at 0x1177e06a8>  units:  [100] dropout: 0.2  rounds 1600auc:   0.954180631077
activation: <function relu at 0x117805488>  units:  [100] dropout: 0.2  rounds 1200auc:   0.937692038213
activation: <function elu at 0x1177e06a8>  units:  [200] dropout: 0.1  rounds 1400auc:   0.951420398892
activation: <function elu at 0x1177e06a8>  units:  [200] dropout: 0.2  rounds 1600auc:   0.949644955429
activation: <function elu at 0x1177e06a8>  units:  [35] dropout: 0.2  rounds 2200auc:   0.951584971559
activation: <function elu at 0x1177e06a8>  units:  [10, 10] dropout: 0.2  rounds 4000auc:   0.93931040964
activation: <function elu at 0x1177e06a8>  units:  [30, 20, 10] dropout: 0.2  rounds 1400auc:   0.948613839282
activation: <function relu at 0x117805488>  units:  [30, 20, 10] dropout: 0.2  rounds 2400auc:   0.948586925253
activation: <function relu at 0x117805488>  units:  [65, 50, 35, 20, 10] dropout: 0.5  rounds 1400auc:   0.933208866981
activation: <function elu at 0x1177e06a8>  units:  [80, 40] dropout: 0.1  rounds 1800auc:   0.948974751994
activation: <function relu at 0x117805488>  units:  [100] dropout: 0.5  rounds 3400auc:   0.936892117984
activation: <function relu at 0x117805488>  units:  [10, 10] dropout: 0.1  rounds 2200auc:   0.960269378542
activation: <function elu at 0x1177e06a8>  units:  [10, 10] dropout: 0.5  rounds 1200auc:   0.925851630461
activation: <function relu at 0x117805488>  units:  [160, 60] dropout: 0.1  rounds 1200auc:   0.939447406458
activation: <function relu at 0x117805488>  units:  [80, 40] dropout: 0.2  rounds 1400auc:   0.950555179053
activation: <function relu at 0x117805488>  units:  [40, 30, 20, 10, 5] dropout: 0.1  rounds 1200auc:   0.935675030311
activation: <function elu at 0x1177e06a8>  units:  [40, 30, 20, 10, 5] dropout: 0.2  rounds 1200auc:   0.932149071951
activation: <function relu at 0x117805488>  units:  [40, 30, 20, 10, 5] dropout: 0.2  rounds 6200auc:   0.943968301451
activation: <function elu at 0x1177e06a8>  units:  [160, 60] dropout: 0.2  rounds 1200auc:   0.948461620595
activation: <function elu at 0x1177e06a8>  units:  [100] dropout: 0.1  rounds 1800auc:   0.952190316597
activation: <function elu at 0x1177e06a8>  units:  [50, 25, 10, 5] dropout: 0.1  rounds 1200auc:   0.95596710488
activation: <function relu at 0x117805488>  units:  [80, 40] dropout: 0.5  rounds 1200auc:   0.934514197371
activation: <function relu at 0x117805488>  units:  [50, 25, 10, 5] dropout: 0.1  rounds 1200auc:   0.945855813165
activation: <function relu at 0x117805488>  units:  [200] dropout: 0.1  rounds 1600auc:   0.947224016491
activation: <function relu at 0x117805488>  units:  [30, 20, 10] dropout: 0.5  rounds 3200auc:   0.94568285744
activation: <function elu at 0x1177e06a8>  units:  [160, 60] dropout: 0.1  rounds 1200auc:   0.934208656963
activation: <function relu at 0x117805488>  units:  [160, 60] dropout: 0.5  rounds 2200auc:   0.949790776518
activation: <function elu at 0x1177e06a8>  units:  [70, 50, 30] dropout: 0.5  rounds 1200auc:   0.946199518548
activation: <function relu at 0x117805488>  units:  [70, 50, 30] dropout: 0.2  rounds 2200auc:   0.947006498194
activation: <function relu at 0x117805488>  units:  [35] dropout: 0.2  rounds 1200auc:   0.944399367123
